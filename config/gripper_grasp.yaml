# Robot parameters
robot:
  model_path: models/franka/franka.urdf #models/kuka_iiwa/kuka_with_gripper.sdf
  max_translation: 0.03
  max_yaw_rotation: 0.15
  max_force: 100
  discrete: False
  step_size: 0.01 #For discrete action space
  yaw_step: 0.1
  num_actions_pad: 2
  # include_robot_height: True


# Experimental setup parameters
scene:
  scene_type: "OnTable"
  # scene_type: "OnFloor"
  data_set: random_urdfs

# Simulation parameters
simulation:
  real_time: False
  visualize: True

sensor:
  camera_info: config/camera_info.yaml
  transform: config/camera_transform.yaml
  encoder_dir: encoder_files/new_gripper_encoder
  visualize: false

  # Randomize camera parameters
  randomize:
    focal_length: 4
    optical_center: 2
    translation: 0.002
    rotation: 0.0349

# Custom shaped reward function parameters
reward:
  custom: True
  shaped: True
  terminal_reward: 10000.
  # lift_success: 1000.
  grasp_reward: 100.
  delta_z_scale: 1000.
  time_penalty: 200.
  table_clearing: True # False # picks only one item. With True the episode terminates after each objects are cleared. Table clearing works better with the curriculum parameter max object [1, 5] 

# Workspace curriculum parameters
curriculum:
  init_lambda: 0.
  n_steps: 8
  success_threshold: 0.7
  window_size: 1000
  extent: [0.01, 0.1]
  robot_height: [0.17, 0.27] #[0.15, 0.25]
  lift_dist: [0.01, 0.1]
  max_objects: [1, 5]
  min_objects: [1, 1]

# Generate new initial states until at least on object is within the FOV
skip_empty_initial_state: False

# Use simplified problem formulation
simplified: False
# Depth + Actuator
depth_observation: True
# RGB + Depth + Actuator
full_observation: False
# Markov decision process parameters
discount_factor: 0.99
time_horizon: 300
# normalize the input and rewards
normalize: True

SAC:
  max_iters: 400
  batch_size: 64
  layers: [64, 64]
  buffer_size: 1000000
  step_size: 0.0003
  tensorboard_logs: !!null
  total_timesteps: 2000000
  save_dir: sac2m_depth_obs

PPO:
  learning_rate: 0.0003
  layers: [64, 64]
  save_dir: ppo_5m
  tensorboard_logs: !!null
  total_timesteps: 5000000
  n_steps: 2000

DQN:
  learning_rate: 0.001
  batch_size: 32
  tensorboard_logs: !!null
  save_dir: DQN4mFull
  total_timesteps: 4000000
  prioritized_replay: True

DDPG:
  save_dir: DDPG
  tensorboard_logs: DDPG
  total_timesteps: 2000000
